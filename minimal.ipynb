{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3551d1c9-214b-411f-a4af-8b8de0f0cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "import peripheral_model\n",
    "import perceptual_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66461dff-c3a3-4ed4-875c-c8f24212bd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_speaker_int torch.Size([2, 433]) torch.float32\n",
      "label_word_int torch.Size([2, 794]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(peripheral_model)\n",
    "importlib.reload(perceptual_model)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_model={},\n",
    "        architecture=[],\n",
    "        input_shape=[2, 65000, 2],\n",
    "        config_random_slice={\"size\": [50, 10000], \"buffer\": [0, 1000]},\n",
    "        device=None,\n",
    "    ):\n",
    "        \"\"\" \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        kwargs_peripheral_model = {\n",
    "            \"sr_input\": config_model[\"kwargs_cochlea\"].get(\"sr_input\", None),\n",
    "            \"sr_output\": config_model[\"kwargs_cochlea\"].get(\"sr_output\", None),\n",
    "            \"config_cochlear_filterbank\": config_model[\"kwargs_cochlea\"].get(\n",
    "                \"config_filterbank\", {}\n",
    "            ),\n",
    "            \"config_ihc_transduction\": config_model[\"kwargs_cochlea\"].get(\n",
    "                \"config_subband_processing\", {}\n",
    "            ),\n",
    "            \"config_ihc_lowpass_filter\": config_model[\"kwargs_cochlea\"].get(\n",
    "                \"kwargs_fir_lowpass_filter_output\", {}\n",
    "            ),\n",
    "            \"config_anf_rate_level\": config_model[\"kwargs_cochlea\"].get(\n",
    "                \"kwargs_sigmoid_rate_level_function\", {}\n",
    "            ),\n",
    "            \"config_anf_spike_generator\": config_model[\"kwargs_cochlea\"].get(\n",
    "                \"kwargs_spike_generator_binomial\", {}\n",
    "            ),\n",
    "            \"config_random_slice\": config_random_slice,\n",
    "        }\n",
    "        assert kwargs_peripheral_model[\"config_ihc_lowpass_filter\"].pop(\n",
    "            \"ihc_filter\", True\n",
    "        )\n",
    "        # print(self.input_shape, json.dumps(kwargs_peripheral_model, indent=4))\n",
    "        self.peripheral_model = peripheral_model.PeripheralModel(\n",
    "            **kwargs_peripheral_model,\n",
    "        )\n",
    "        self.perceptual_model = perceptual_model.PerceptualModel(\n",
    "            architecture=architecture,\n",
    "            input_shape=self.peripheral_model(torch.zeros(self.input_shape)).shape,\n",
    "            heads=config_model[\"n_classes_dict\"],\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \"\"\"\n",
    "        return self.perceptual_model(self.peripheral_model(x))\n",
    "\n",
    "\n",
    "dir_model = \"../phaselocknet/models/sound_localization/simplified_IHC3000_delayed_integration/arch01\"\n",
    "input_shape = [2, 65000, 2]\n",
    "config_random_slice = {\"size\": [50, 10000], \"buffer\": [0, 1000]}\n",
    "\n",
    "dir_model = \"../phaselocknet/models/spkr_word_recognition/simplified_IHC3000/arch0_0000\"\n",
    "input_shape = [2, 40000]\n",
    "config_random_slice = {\"size\": [50, 20000], \"buffer\": [0, 0]}\n",
    "\n",
    "# dir_model = \"../phaselocknet/models/spkr_word_recognition/IHC3000/arch0_0000\"\n",
    "# input_shape = [2, 3, 50, 20000]\n",
    "# config_random_slice = {}\n",
    "\n",
    "# dir_model = \"../phaselocknet/models/sound_localization/IHC3000_delayed_integration/arch01\"\n",
    "# input_shape = [2, 3, 50, 13000, 2]\n",
    "# config_random_slice = {\"size\": [50, 10000], \"buffer\": [0, 1000]}\n",
    "\n",
    "with open(os.path.join(dir_model, \"config.json\")) as f:\n",
    "    config_model = json.load(f)\n",
    "with open(os.path.join(dir_model, \"arch.json\")) as f:\n",
    "    architecture = json.load(f)\n",
    "\n",
    "model = Model(\n",
    "    config_model=config_model,\n",
    "    architecture=architecture,\n",
    "    input_shape=input_shape,\n",
    "    config_random_slice=config_random_slice,\n",
    ")\n",
    "\n",
    "for k, v in model(torch.zeros(model.input_shape)).items():\n",
    "    print(k, v.shape, v.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66f5fac4-019c-4e59-98e3-1158f57f4fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body.block0_conv.weight [32  3  2 42]\n",
      "body.block0_conv.bias [32]\n",
      "body.block0_norm.weight [32]\n",
      "body.block0_norm.bias [32]\n",
      "body.block1_conv.weight [64 32  2 18]\n",
      "body.block1_conv.bias [64]\n",
      "body.block1_norm.weight [64]\n",
      "body.block1_norm.bias [64]\n",
      "body.block2_conv.weight [128  64   6   6]\n",
      "body.block2_conv.bias [128]\n",
      "body.block2_norm.weight [128]\n",
      "body.block2_norm.bias [128]\n",
      "body.block3_conv.weight [256 128   6   6]\n",
      "body.block3_conv.bias [256]\n",
      "body.block3_norm.weight [256]\n",
      "body.block3_norm.bias [256]\n",
      "body.block4_conv.weight [512 256   8   8]\n",
      "body.block4_conv.bias [512]\n",
      "body.block4_norm.weight [512]\n",
      "body.block4_norm.bias [512]\n",
      "body.block5_conv.weight [512 512   6   6]\n",
      "body.block5_conv.bias [512]\n",
      "body.block5_norm.weight [512]\n",
      "body.block5_norm.bias [512]\n",
      "body.block6_conv.weight [512 512   8   8]\n",
      "body.block6_conv.bias [512]\n",
      "body.block6_norm.weight [512]\n",
      "body.block6_norm.bias [512]\n",
      "body.fc_intermediate_dense.weight [  512 35840]\n",
      "body.fc_intermediate_dense.bias [512]\n",
      "body.fc_intermediate_norm.weight [512]\n",
      "body.fc_intermediate_norm.bias [512]\n",
      "head.label_speaker_int.fc_output.weight [433 512]\n",
      "head.label_speaker_int.fc_output.bias [433]\n",
      "head.label_word_int.fc_output.weight [794 512]\n",
      "head.label_word_int.fc_output.bias [794]\n"
     ]
    }
   ],
   "source": [
    "list_layer_name = []\n",
    "for n, p in model.perceptual_model.named_parameters():\n",
    "    print(n, np.array(p.shape))\n",
    "    name = n.replace(\".bias\", \"\").replace(\".weight\", \"\")\n",
    "    if name not in list_layer_name:\n",
    "        list_layer_name.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7aa61325-b2ae-445f-a017-fa6cee520445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensorflow_checkpoint(filename):\n",
    "    reader = tf.train.load_checkpoint(filename)\n",
    "    shapes = reader.get_variable_to_shape_map()\n",
    "    dtypes = reader.get_variable_to_dtype_map()\n",
    "    tf_state_dict = {}\n",
    "    for k in shapes:\n",
    "        if (\"layer_with_weights\" in k) and (\"OPTIMIZER_SLOT\" not in k):\n",
    "            tf_state_dict[k.replace(\"/.ATTRIBUTES/VARIABLE_VALUE\", \"\")] = reader.get_tensor(k)\n",
    "    return tf_state_dict\n",
    "\n",
    "filename = os.path.join(dir_model, \"ckpt_BEST\")\n",
    "tf_state_dict = load_tensorflow_checkpoint(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4e99113-2ae9-4fc1-b584-b639dfc3c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_state_dict = {}\n",
    "for k, v in sorted(tf_state_dict.items()):\n",
    "    layer_index = int(k[k.find(\"-\") + 1 : k.find(\"/\")])\n",
    "    layer_name = list_layer_name[layer_index]\n",
    "    name = \"{}.{}\".format(layer_name, \"bias\" if \"/b\" in k else \"weight\")\n",
    "    if v.ndim == 2:\n",
    "        torch_state_dict[name] = torch.tensor(np.transpose(v, [1, 0]))\n",
    "    elif v.ndim == 4:\n",
    "        torch_state_dict[name] = torch.tensor(np.transpose(v, [3, 2, 0, 1]))\n",
    "    else:\n",
    "        torch_state_dict[name] = torch.tensor(v)\n",
    "for n, p in model.perceptual_model.named_parameters():\n",
    "    assert n in torch_state_dict\n",
    "    if not torch_state_dict[n].shape == p.shape:\n",
    "        print(n, p.shape, torch_state_dict[n].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dbd9ef3c-cbfb-47b4-bd3d-b8953e2ae60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['body.block0_pool.weight', 'body.block1_pool.weight', 'body.block2_pool.weight', 'body.block3_pool.weight', 'body.block4_pool.weight', 'body.block5_pool.weight', 'body.block6_pool.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.perceptual_model.load_state_dict(torch_state_dict, strict=False, assign=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0dbefbd4-65f3-4d1e-813b-2157b3bcc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(torch.zeros(model.input_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b27ef-a6b1-4839-9eb5-94d100e1733f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
